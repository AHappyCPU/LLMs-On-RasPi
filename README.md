# Just for fun, how far can you push high-level memory efficiency when running transformer architectures? 

A good test of this is seeing how efficient you can make an LLM run on barebones hardware, and seeing how large you can scale up those LLMs before things break. 

The Raspberry Pi is a great platform for this! To emulate the results, get a RasPi with 4GB of RAM and put in 32GB flash storage. 


Largest model to date: 

32B Deepseek R1 

Most performant model: 

16B Deepseek R1 at 1 token per second (these are considered good numbers). 

